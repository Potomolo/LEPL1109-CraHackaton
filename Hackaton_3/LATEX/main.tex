\documentclass [a4paper, 12pt] {article}

\input{preamble.tex}
\begin{document}

    \begin{center}
        \Large
        \centering
        \maintitle{LEPL1109 - Statistics and Data Sciences}
        \textsc{\textbf{HACKATHON 3 - Bias in Clustering Algorithms}}\\
        \vspace{0.1cm}
         Group n°16\hfill December 22, 2024 \\
       \noindent\hrulefill
    \end{center}
    \vspace*{0.5cm}

    % ------------------------------------------------------------
    % FILL WITH YOUR NAMES: BEGIN
    % ------------------------------------------------------------
    \hspace*{-0.75cm}
    \fbox{\parbox{\textwidth}{
    \begin{tabularx}{\textwidth}{X|X|c}
       \textbf{Lastname}  & \textbf{Firstname} & \textbf{Noma} \\
       \hline
        Decaluwé & Maxime & 50802200 \\
        Defrenne & Simon & 42242200 \\
        Mil-Homens Cavaco & Mathieu & 38282200 \\
        Peiffer & Thibaut & 47352200 \\ 
        Roekens & Raphaël & 70732200 \\ 
        Starck & Robin & 88952200 \\ 
    \end{tabularx}
    }}
    % ------------------------------------------------------------
    % FILL WITH YOUR NAMES: END
    % ------------------------------------------------------------

    \vspace*{0.5cm}
    
    \hrule
    
    \vspace*{0.5cm}
    
    Please carefully read the following guidelines:
    
    \begin{itemize}
        \item Answer in English, with complete sentences and correct grammar. Feel free to use grammar checker tools such as \href{https://languagetool.org/fr}{LanguageTools}.
        \item Do not modify questions, and input all answers inside \la{\begin{answer}...\end{answer}} environments.
        \item Each question should be followed by an answer.
        \item You are allowed (and often encouraged) to add figures that support your answers, provided that you explain how they support your claims.
        \item Clearly cite every source of information (even for pictures!).
        \item Whenever possible, use the \texttt{.pdf} format when you export your images (this usually makes your report look prettier).
    \end{itemize}

    {
    \hypersetup{allcolors=black}
    }   
    \clearpage

\renewcommand{\arraystretch}{0.84}

\begin{question}{1.1: Removing unnecessary features}
    Can you already, a priori, detect that some features are useless? If yes, list those (useless) features and explain your choice. If not, then explain why it is better to wait.

    Generally speaking, is it a good idea to remove a feature based on \emph{a priori} knowledge, or doesn't it alter the final outcome?
\end{question}
\begin{answer}\color{blue} 
 We can a priori identify certain features as non-essential. Firstly, irrelevant variables such as "name," "first," "last," "c\_case\_number," "c\_charge\_desc," and "r\_case\_number" do not influence an individual's likelihood to recidivate. Therefore, these can be excluded from the dataset without affecting the model's predictive performance.

Additionally, some features may introduce data leakage if retained. Specifically, "decile\_score," "is\_recid," "is\_violent\_recid," "decile\_score.1," "score\_text," "v\_decile\_score," and "v\_score\_text" represent predictions of recidivism probabilities generated by external entities. Including these in our model would inadvertently incorporate external predictive information, thereby biasing our results.

Furthermore, variables such as "type\_of\_assessment" and "v\_type\_of\_assessment," which remain constant across all records, offer no discriminatory value and can be safely removed.

In general, removing features based on a priori knowledge can enhance model performance by eliminating noise and preventing overfitting. However, it is crucial to ensure that the removal does not discard potentially valuable information.
% ------------------------------------------------------------
% TO FILL 
% ------------------------------------------------------------
\end{answer}

\begin{question}{1.2: Handling missing data}
    Given the dataset and the amount / type of missing information, what strategy do you propose to follow regarding missing data (NaNs) ? Justify briefly your choice.
\end{question}
\begin{answer}\color{blue} 
% ------------------------------------------------------------
% TO FILL 
% ------------------------------------------------------------
When a feature contains a substantial amount of missing data (NaNs), our strategy is to remove that feature from the dataset. This decision is based on the fact that features with excessive missing values can hinder the model's ability to learn effectively, leading to decreased performance and reliability. By excluding such features, we ensure that the dataset remains robust and that the model is trained on complete and meaningful information, thereby enhancing its predictive accuracy.

\end{answer}

\begin{question}{1.3: New features}
    What features have you added? If a particular manipulation has been applied, please explain.
\end{question}
\begin{answer}\color{blue} 
% ------------------------------------------------------------
% TO FILL 
% ------------------------------------------------------------
We performed feature engineering to enhance the dataset by creating more informative and relevant variables.

Firstly, we calculated the duration of incarceration (d\_time\_jail) by subtracting the date of entry from the date of release. Similarly, we computed the duration of custody (d\_custody) by calculating the difference between the custody entry and release dates. These duration-based features provide a more meaningful representation of an individual's time in jail and custody compared to the raw dates alone.

Secondly, we transformed the screening dates ("compas\_screening\_date","screening\_date","v\_screening\_date") into the individuals' ages at the time of screening (measured in days) by combining the screening dates with their respective dates of birth ("dob"). This transformation allows us to incorporate age as a continuous variable, offering better insights and improving the predictive performance of our models.
\end{answer}

\begin{question}{2.1: (Im)Balanced dataset ?}
Is the dataset imbalanced ? What could be the consequences in terms of fairness i.e. in terms of the model performing equally well across all groups ?
\end{question}
\begin{answer}\color{blue} 
% ------------------------------------------------------------
% TO FILL 
% ------------------------------------------------------------
\end{answer}

\begin{question}{2.2: Principal Component Analysis}
Do all features have the same importance? If no, which features are less important, and why ? You can use all other graphs from the visualization part to justify your answer.
\end{question}
\begin{answer}\color{blue}
% ------------------------------------------------------------
% TO FILL 
% ------------------------------------------------------------
\end{answer}

\begin{question}{3.1: Number of clusters}
Accounting for all features, what do you think is the ideal number of clusters ? What will happen if too many or too few clusters are chosen
\end{question}
\begin{answer} \color{blue}
% ------------------------------------------------------------
% TO FILL 
% ------------------------------------------------------------
\end{answer}

\begin{question}{3.2: Quality of the clustering}
You considered three different measures for the quality of the clustering: the first one is the silhouette score and is oblivious to the true labels: it is a truly unsupervised metric. The second and third metric use the true label to assess the quality of the clustering. Based on this observation,
\begin{enumerate}
    \item Comment on the evolution of each metric according to the number of clusters.
    \item Comment on what do you now think is the ideal number of clusters.
\end{enumerate}
\end{question}
\begin{answer}\color{blue}
% ------------------------------------------------------------
% TO FILL 
% ------------------------------------------------------------
\end{answer}

\begin{question}{4.1: Fairness of your model}
You considered two different measures for the fairness of your model and checked for various variants of your algorithm (number of clusters) the value of these fairness metrics.

Is your algorithm unfair ? If yes, which ethnic group is penalized by the unfairness of your model ?
\end{question}
\begin{answer}\color{blue}
% ------------------------------------------------------------
% TO FILL 
% ------------------------------------------------------------
\end{answer}

\begin{question}{4.2: Presence of the sensitive features in the dataset [BONUS]}
In Cell 1.5, you removed the sensitive features from your dataset before building your algorithm. Yet, you may have noticed unfairness in your algorithm.
\begin{enumerate}
    \item Provide reasons why it is not necessarily enough to remove sensitive features from your dataset if you want to have fair predictions.
    \item Compute FPR and Demographic Parity for your algorithm when trained on the full dataset. Is the fairness of your classifier worse ?
\end{enumerate}
\end{question}
\begin{answer}\color{blue}
% ------------------------------------------------------------
% TO FILL 
% ------------------------------------------------------------
\end{answer}

\begin{question}{5.1: Visualization}
Produce a clear, clean figure expressing a result or giving an overall vision of your work for this hackathon. Please feel free to do as you wish. Be original! The clarity, content and description of your figure will be evaluated.
\end{question}
\begin{answer}\color{blue}
% ------------------------------------------------------------
% TO FILL 
% ------------------------------------------------------------
\end{answer}

\clearpage

\end{document}
